{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "150e0ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7383ec4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "viterbinet                               --                        --\n",
       "├─LSTM: 1-1                              [5000, 1, 100]            41,200\n",
       "├─Linear: 1-2                            [5000, 50]                5,050\n",
       "├─Linear: 1-3                            [5000, 16]                816\n",
       "==========================================================================================\n",
       "Total params: 47,066\n",
       "Trainable params: 47,066\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 235.33\n",
       "==========================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 6.64\n",
       "Params size (MB): 0.19\n",
       "Estimated Total Size (MB): 6.85\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class viterbinet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_unit_size, num_layers, class_size):\n",
    "        super(viterbinet, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_unit_size = hidden_unit_size\n",
    "\n",
    "        # set the ViterbiNet architecture.\n",
    "        # 1 x 100 , 100 x 50, 50 x 16\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_unit_size,\n",
    "                            num_layers=num_layers, batch_first=True)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=hidden_unit_size, out_features=int(hidden_unit_size/2))\n",
    "\n",
    "        self.fc2 = nn.Linear(in_features=int(hidden_unit_size/2), out_features=class_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = Variable(torch.zeros(1, 5000, 100))  # hidden state initialized with zeros\n",
    "        c_0 = Variable(torch.zeros(1, 5000, 100))  # internal state initialized with zeros\n",
    "        # Propagate input through LSTM\n",
    "        x = x.view(5000, 1, 1)\n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0))  # lstm with input, hidden, and internal state at current timestamp, returns a new hidden state, current state, and output.\n",
    "        #lstm_out, _ = self.lstm(x)\n",
    "        hn = hn.view(-1, self.hidden_unit_size)  # reshape the output so that it can pass to a dense layer\n",
    "        out = torch.tanh(hn)  # lstm activation function (tanh)\n",
    "        out = self.fc1(out)  # first Dense\n",
    "        out = F.relu(out)  # relu\n",
    "        out = self.fc2(out)  # Final Output\n",
    "        out = F.softmax(out, dim=1)  # softmax\n",
    "        return out\n",
    "    \n",
    "net = viterbinet(input_size=1, hidden_unit_size=100, num_layers=1, class_size=16)\n",
    "summary(net, input_size=(1,1,5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0b9d39a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CNN                                      --                        --\n",
       "├─Conv1d: 1-1                            [8, 32, 5000]             256\n",
       "├─MaxPool1d: 1-2                         [8, 32, 2500]             --\n",
       "├─Conv1d: 1-3                            [8, 64, 2500]             14,400\n",
       "├─MaxPool1d: 1-4                         [8, 64, 1250]             --\n",
       "├─Conv1d: 1-5                            [8, 128, 1250]            57,472\n",
       "├─MaxPool1d: 1-6                         [8, 128, 625]             --\n",
       "├─Linear: 1-7                            [5000, 16]                2,064\n",
       "==========================================================================================\n",
       "Total params: 74,192\n",
       "Trainable params: 74,192\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 883.28\n",
       "==========================================================================================\n",
       "Input size (MB): 0.16\n",
       "Forward/backward pass size (MB): 31.36\n",
       "Params size (MB): 0.30\n",
       "Estimated Total Size (MB): 31.82\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1d = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=7, stride=1, padding=3)\n",
    "        self.conv1d2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=7, stride=1, padding=3)\n",
    "        self.conv1d3 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=7, stride=1, padding=3)\n",
    "        self.maxpool = nn.MaxPool1d(2)\n",
    "        self.fc = nn.Linear(128, 16)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # layer 1\n",
    "        x = self.conv1d(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        # # layer2\n",
    "        x = self.conv1d2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        # layer3\n",
    "        x = self.conv1d3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        # flatten\n",
    "        #print(x.view(-1,x.size(0)).shape)\n",
    "        #x = x.flatten()\n",
    "        x = x.view(-1, 128)\n",
    "        #x = x.view(x.size(0), -1)\n",
    "        # feed to a fully-connected layer\n",
    "        x = self.fc(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "net = CNN()\n",
    "summary(net, input_size=(8,1,5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "406349a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "multiLSTM                                --                        --\n",
       "├─LSTM: 1-1                              [5000, 1, 100]            41,200\n",
       "├─LSTM: 1-2                              [5000, 1, 100]            41,200\n",
       "├─Linear: 1-3                            [5000, 16]                1,616\n",
       "==========================================================================================\n",
       "Total params: 84,016\n",
       "Trainable params: 84,016\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 420.08\n",
       "==========================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 8.64\n",
       "Params size (MB): 0.34\n",
       "Estimated Total Size (MB): 9.00\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class multiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_unit_size, num_layers, class_size):\n",
    "        super(multiLSTM, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_unit_size = hidden_unit_size\n",
    "\n",
    "        # set the ViterbiNet architecture.\n",
    "        # 1 x 100 , 100 x 100, 100 x 100, 100 x 16\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_unit_size,\n",
    "                            num_layers=num_layers, batch_first=True)\n",
    "\n",
    "        self.lstm2 = nn.LSTM(input_size=input_size, hidden_size=hidden_unit_size,\n",
    "                            num_layers=num_layers, batch_first=True)\n",
    "\n",
    "\n",
    "        self.fc2 = nn.Linear(in_features=hidden_unit_size, out_features=class_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        np.random.seed(9001)\n",
    "        if torch.cuda.is_available():\n",
    "            h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_unit_size).cuda())\n",
    "        else:\n",
    "            h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_unit_size))\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_unit_size).cuda())\n",
    "        else:\n",
    "            c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_unit_size))\n",
    "\n",
    "\n",
    "\n",
    "        # Propagate input through LSTM\n",
    "        output, (h1, c1) = self.lstm(x, (h_0, c_0))  # lstm with input, hidden, and internal state at current timestamp, returns a new hidden state, current state, and output.\n",
    "        output2, (h2, c2) = self.lstm2(x, (h1, c1))\n",
    "        # output3, (h3, c3) = self.lstm3(x, (h2, c2))\n",
    "        hx = h2.view(-1, self.hidden_unit_size)  # reshape the output so that it can pass to a dense layer\n",
    "\n",
    "        out = torch.tanh(hx)  # lstm activation function (tanh)\n",
    "        out = self.fc2(out)  # Final Output\n",
    "        #out = F.softmax(out, dim=1)  # softmax\n",
    "        out = torch.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "net = multiLSTM(input_size=1, hidden_unit_size=100, num_layers=1, class_size=16)\n",
    "summary(net, (5000,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d86af433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "multiGRU                                 --                        --\n",
       "├─GRU: 1-1                               [5000, 1, 100]            30,900\n",
       "├─GRU: 1-2                               [5000, 1, 100]            30,900\n",
       "├─Linear: 1-3                            [5000, 16]                1,616\n",
       "==========================================================================================\n",
       "Total params: 63,416\n",
       "Trainable params: 63,416\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 317.08\n",
       "==========================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 8.64\n",
       "Params size (MB): 0.25\n",
       "Estimated Total Size (MB): 8.91\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class multiGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_unit_size, num_layers, class_size):\n",
    "        super(multiGRU, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_unit_size = hidden_unit_size\n",
    "\n",
    "        # set the ViterbiNet architecture.\n",
    "        # 1 x 100 , 100 x 50, 50 x 16\n",
    "\n",
    "        self.gru = nn.GRU(input_size=input_size, hidden_size=hidden_unit_size,\n",
    "                          num_layers=num_layers, batch_first=True)\n",
    "\n",
    "        self.gru2 = nn.GRU(input_size=input_size, hidden_size=hidden_unit_size,\n",
    "                           num_layers=num_layers, batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(in_features=hidden_unit_size, out_features=class_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if torch.cuda.is_available():\n",
    "            h0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_unit_size).cuda())\n",
    "        else:\n",
    "            h0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_unit_size))\n",
    "\n",
    "        output, h1 = self.gru(x, h0.detach())\n",
    "        output2, h2 = self.gru2(x, h1.detach())\n",
    "        out = output2[:, -1, :]\n",
    "\n",
    "        out = self.fc(out)\n",
    "        #out = F.softmax(out, dim=1)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "net = multiGRU(input_size=1, hidden_unit_size=100, num_layers=1, class_size=16)\n",
    "summary(net, (5000,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee96e73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MultiHeadAttention                       --                        --\n",
       "├─Linear: 1-1                            [5000, 1, 150]            22,650\n",
       "├─Linear: 1-2                            [5000, 1, 150]            22,650\n",
       "├─Linear: 1-3                            [5000, 1, 150]            22,650\n",
       "├─ScaleDotProductAttention: 1-4          [5000, 5, 1, 30]          --\n",
       "│    └─Softmax: 2-1                      [5000, 5, 1, 1]           --\n",
       "├─Linear: 1-5                            [5000, 16]                2,416\n",
       "==========================================================================================\n",
       "Total params: 70,366\n",
       "Trainable params: 70,366\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 351.83\n",
       "==========================================================================================\n",
       "Input size (MB): 3.00\n",
       "Forward/backward pass size (MB): 18.64\n",
       "Params size (MB): 0.28\n",
       "Estimated Total Size (MB): 21.92\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "##**Taken from##\n",
    "##**** https://github.com/hyunwoongko/transformer** ##\n",
    "\n",
    "class ScaleDotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    compute scale dot product attention\n",
    "    Query : given sentence that we focused on (decoder)\n",
    "    Key : every sentence to check relationship with Qeury(encoder)\n",
    "    Value : every sentence same with Key (encoder)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ScaleDotProductAttention, self).__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None, e=1e-12):\n",
    "        # input is 4 dimension tensor\n",
    "        # [batch_size, head, length, d_tensor]\n",
    "        batch_size, head, length, d_tensor = k.size()\n",
    "\n",
    "        # 1. dot product Query with Key^T to compute similarity\n",
    "        k_t = k.transpose(2, 3)  # transpose\n",
    "        score = (q @ k_t) / math.sqrt(d_tensor)  # scaled dot product\n",
    "\n",
    "        # 2. apply masking (opt)\n",
    "        if mask is not None:\n",
    "            score = score.masked_fill(mask == 0, -e)\n",
    "\n",
    "        # 3. pass them softmax to make [0, 1] range\n",
    "        score = self.softmax(score)\n",
    "\n",
    "        # 4. multiply with Value\n",
    "        v = score @ v\n",
    "\n",
    "        return v, score\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model=150, n_head=5):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_head = n_head\n",
    "        self.attention = ScaleDotProductAttention()\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        #self.w_k = nn.Linear(d_model, d_model)\n",
    "        #elf.w_v = nn.Linear(d_model, d_model)\n",
    "        #self.w_concat = nn.Linear(d_model, 16)\n",
    "        self.fc = nn.Linear(d_model, 16)\n",
    "\n",
    "    def forward(self, q, mask=None):\n",
    "        # 1. dot product with weight matrices\n",
    "        q = self.w_q(q)\n",
    "        q = F.relu(q)\n",
    "        q = self.w_k(q)\n",
    "        q = F.relu(q)\n",
    "        q = self.w_v(q)\n",
    "        q = F.relu(q)\n",
    "\n",
    "        # 2. split tensor by number of heads\n",
    "        q = self.split(q)\n",
    "\n",
    "        # 3. do scale dot product to compute similarity\n",
    "        out, attention = self.attention(q,q,q, mask=mask)\n",
    "\n",
    "        # 4. concat and pass to linear layer\n",
    "        out = self.concat(out)\n",
    "        out = out.view(-1, 150)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # 5. visualize attention map\n",
    "        # TODO : we should implement visualization\n",
    "\n",
    "        return out\n",
    "\n",
    "    def split(self, tensor):\n",
    "        \"\"\"\n",
    "        split tensor by number of head\n",
    "        :param tensor: [batch_size, length, d_model]\n",
    "        :return: [batch_size, head, length, d_tensor]\n",
    "        \"\"\"\n",
    "        batch_size, length, d_model = tensor.size()\n",
    "\n",
    "        d_tensor = d_model // self.n_head\n",
    "        tensor = tensor.view(batch_size, length, self.n_head, d_tensor).transpose(1, 2)\n",
    "        # it is similar with group convolution (split by number of heads)\n",
    "\n",
    "        return tensor\n",
    "\n",
    "    def concat(self, tensor):\n",
    "        \"\"\"\n",
    "        inverse function of self.split(tensor : torch.Tensor)\n",
    "        :param tensor: [batch_size, head, length, d_tensor]\n",
    "        :return: [batch_size, length, d_model]\n",
    "        \"\"\"\n",
    "        batch_size, head, length, d_tensor = tensor.size()\n",
    "        d_model = head * d_tensor\n",
    "\n",
    "        tensor = tensor.transpose(1, 2).contiguous().view(batch_size, length, d_model)\n",
    "        return tensor\n",
    "    \n",
    "net = MultiHeadAttention()\n",
    "summary(net, (5000,1,150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4009d853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "MyLSTM                                   --                        --\n",
       "├─LSTM: 1-1                              [5000, 1, 100]            41,200\n",
       "├─Attention: 1-2                         [5000, 100]               200\n",
       "├─Attention: 1-3                         [5000, 100]               200\n",
       "├─Attention: 1-4                         [5000, 100]               200\n",
       "├─Linear: 1-5                            [5000, 16]                1,616\n",
       "==========================================================================================\n",
       "Total params: 43,416\n",
       "Trainable params: 43,416\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 215.58\n",
       "==========================================================================================\n",
       "Input size (MB): 0.02\n",
       "Forward/backward pass size (MB): 16.64\n",
       "Params size (MB): 0.17\n",
       "Estimated Total Size (MB): 16.83\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "\n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.xavier_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "\n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        feature_dim = self.feature_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(x.contiguous().view(-1, feature_dim),self.weight)\n",
    "            #torch.bmm(x, self.weight.unsqueeze(0).repeat(5000, 1, 1))\n",
    "\n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "\n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n",
    "\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1)\n",
    "\n",
    "\n",
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, hidden_dim=100, lstm_layer=1):\n",
    "        super(MyLSTM, self).__init__()\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_size=1, hidden_size=100,\n",
    "                             num_layers=1, batch_first=True)\n",
    "\n",
    "        self.atten1 = Attention(hidden_dim, 100)  # 2 is bidrectional\n",
    "        self.atten2 = Attention(hidden_dim, 100)  # 2 is bidrectional\n",
    "        self.atten3 = Attention(hidden_dim, 100)  # 2 is bidrectional\n",
    "        \n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=100, out_features=16)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            h_0 = Variable(torch.zeros(1, x.size(0), 100).cuda())\n",
    "        else:\n",
    "            h_0 = Variable(torch.zeros(1, x.size(0), 100))\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            c_0 = Variable(torch.zeros(1, x.size(0), 100).cuda())\n",
    "        else:\n",
    "            c_0 = Variable(torch.zeros(1, x.size(0), 100))\n",
    "\n",
    "        out1, (hn, cn) = self.lstm1(x, (h_0, c_0))\n",
    "        #hx = hn.view(-1, 100)\n",
    "        ans1 = self.atten1(out1)  # skip connect\n",
    "        ans2 = self.atten2(out1)\n",
    "        ans3 = self.atten3(out1)\n",
    "        z = ans1 + ans2 + ans3\n",
    "\n",
    "        out = self.fc1(z)\n",
    "        out = torch.sigmoid(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    \n",
    "net = MyLSTM()\n",
    "summary(net, (5000,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18095fba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
